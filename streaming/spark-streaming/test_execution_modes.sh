#!/bin/bash
# Test script to demonstrate both local and cluster execution modes

echo "ğŸ§ª Testing Spark Streaming Execution Modes"
echo "=========================================="

cd /Users/kimanthi/projects/ecommerce-streaming-analytics/streaming/spark-streaming

echo ""
echo "ğŸ“‹ Available options:"
./run_streaming.sh --help

echo ""
echo "ğŸ” Checking Spark cluster status..."
if curl -s --connect-timeout 5 http://localhost:7080 > /dev/null; then
    echo "âœ… Spark cluster is accessible at http://localhost:7080"
    echo "ğŸ“Š Cluster info:"
    curl -s http://localhost:7080 | grep -E "(Workers|Memory|Cores)" | head -3 || echo "   (Could not parse cluster details)"
else
    echo "âŒ Spark cluster not accessible"
fi

echo ""
echo "ğŸ” Checking SPARK_HOME for local execution..."
if [ -n "$SPARK_HOME" ]; then
    echo "âœ… SPARK_HOME is set: $SPARK_HOME"
    if [ -f "$SPARK_HOME/bin/spark-submit" ]; then
        echo "âœ… spark-submit found"
    else
        echo "âŒ spark-submit not found in SPARK_HOME"
    fi
else
    echo "âš ï¸  SPARK_HOME not set (required for --local mode)"
fi

echo ""
echo "ğŸ“ Usage recommendations:"
echo "  - Use './run_streaming.sh --cluster' to leverage Docker Spark cluster"
echo "  - Use './run_streaming.sh --local' if you have local Spark installation"
echo "  - Cluster mode is recommended for testing the complete pipeline"

echo ""
echo "ğŸš€ To start streaming:"
echo "  ./run_streaming.sh --cluster    # Submit to Docker cluster"
echo "  ./run_streaming.sh --local      # Run locally"