# Data Lake Architecture Update

## Overview

Migrated from multiple separate buckets to a single unified bucket with organized subfolder structure. This follows industry best practices and simplifies management.

## Before vs After

### âŒ Before (Multiple Buckets)

```
MinIO:
â”œâ”€â”€ warehouse/           # Separate bucket
â”œâ”€â”€ bronze/             # Separate bucket
â”œâ”€â”€ silver/             # Separate bucket
â”œâ”€â”€ gold/               # Separate bucket
â”œâ”€â”€ checkpoints/        # Separate bucket
â”œâ”€â”€ logs/               # Separate bucket
â””â”€â”€ scripts/            # Separate bucket
```

### âœ… After (Single Bucket + Subfolders)

```
MinIO:
â””â”€â”€ data-lake/
    â”œâ”€â”€ warehouse/      # Iceberg warehouse metadata
    â”œâ”€â”€ bronze/         # Raw ingested data
    â”œâ”€â”€ silver/         # Cleaned, validated data
    â”œâ”€â”€ gold/           # Analytics-ready aggregated data
    â”œâ”€â”€ checkpoints/    # Spark streaming checkpoints
    â”œâ”€â”€ logs/           # Application and audit logs
    â””â”€â”€ scripts/        # ETL scripts and utilities
```

## Benefits of Single Bucket Architecture

### ğŸ¯ **Management**

- **Simplified Permissions**: Single bucket-level IAM policies
- **Unified Monitoring**: One place to track all data lake activity
- **Easier Backup/Restore**: Single bucket lifecycle management
- **Cost Tracking**: Consolidated billing and storage analytics

### ğŸ”’ **Security**

- **Centralized Access Control**: Folder-level permissions within single bucket
- **Consistent Encryption**: Single encryption policy for entire data lake
- **Audit Trail**: Unified logging for all data lake operations

### ğŸš€ **Operations**

- **Cleaner URLs**: `s3a://data-lake/bronze/` vs `s3a://bronze/`
- **Industry Standard**: Matches AWS, Azure, GCP data lake patterns
- **Scalability**: Easier to add new layers (e.g., `platinum/`, `archive/`)

## Configuration Changes

### MinIO Bucket Creation

```bash
# Before: Multiple buckets
mc mb myminio/warehouse
mc mb myminio/bronze
mc mb myminio/silver
# ... etc

# After: Single bucket
mc mb myminio/data-lake
```

### Spark Configuration

```bash
# Before: Multiple warehouse paths
--conf spark.sql.catalog.local.warehouse=s3a://warehouse/
--conf spark.sql.catalog.iceberg.warehouse=s3a://bronze/iceberg-warehouse/

# After: Unified warehouse path
--conf spark.sql.catalog.local.warehouse=s3a://data-lake/warehouse/
--conf spark.sql.catalog.iceberg.warehouse=s3a://data-lake/warehouse/
```

### Checkpoint Locations

```python
# Before: Local temp directories
.option("checkpointLocation", "/tmp/checkpoints/bronze")

# After: S3 persistent storage
.option("checkpointLocation", "s3a://data-lake/checkpoints/bronze")
```

## File Organization Best Practices

### ğŸ“ **Folder Structure Guidelines**

```
data-lake/
â”œâ”€â”€ warehouse/
â”‚   â”œâ”€â”€ bronze.db/          # Iceberg database metadata
â”‚   â”œâ”€â”€ silver.db/          # Iceberg database metadata
â”‚   â””â”€â”€ gold.db/            # Iceberg database metadata
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ raw_events/         # Raw event data
â”‚   â”œâ”€â”€ page_views/         # Page view events
â”‚   â””â”€â”€ purchases/          # Purchase events
â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ cleaned_events/     # Validated events
â”‚   â”œâ”€â”€ user_sessions/      # Sessionized data
â”‚   â””â”€â”€ product_catalog/    # Clean product data
â”œâ”€â”€ gold/
â”‚   â”œâ”€â”€ daily_sales/        # Daily aggregations
â”‚   â”œâ”€â”€ user_analytics/     # User behavior metrics
â”‚   â””â”€â”€ product_performance/ # Product metrics
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ bronze/             # Bronze streaming checkpoints
â”‚   â”œâ”€â”€ silver_page_views/  # Silver page views checkpoints
â”‚   â””â”€â”€ silver_purchases/   # Silver purchases checkpoints
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ spark/              # Spark application logs
â”‚   â”œâ”€â”€ dagster/            # Dagster pipeline logs
â”‚   â””â”€â”€ kafka/              # Kafka consumer logs
â””â”€â”€ scripts/
    â”œâ”€â”€ etl/                # ETL job scripts
    â”œâ”€â”€ maintenance/        # Maintenance scripts
    â””â”€â”€ monitoring/         # Monitoring utilities
```

## Implementation Status

### âœ… **Completed Updates**

- [x] Docker Compose bucket creation
- [x] Spark run script configuration
- [x] Real-time streaming checkpoint paths
- [x] Bucket creation utility script
- [x] Documentation updates

### ğŸ¯ **Key Paths Updated**

| Component   | Old Path                    | New Path                                |
| ----------- | --------------------------- | --------------------------------------- |
| Warehouse   | `s3a://warehouse/`          | `s3a://data-lake/warehouse/`            |
| Checkpoints | `/tmp/checkpoints/`         | `s3a://data-lake/checkpoints/`          |
| Raw Events  | `iceberg.bronze.raw_events` | `iceberg.bronze.raw_events` (unchanged) |

### ğŸ”„ **Backward Compatibility**

- Iceberg table names remain unchanged (`iceberg.bronze.*`, `iceberg.silver.*`)
- Logical schema organization preserved
- Only physical storage paths updated

## Migration Steps

### For New Deployments

1. Start services: `docker-compose up -d`
2. Verify bucket: Access http://localhost:9001
3. Run streaming: `./run_streaming.sh --cluster`

### For Existing Deployments

1. **Backup existing data** (if any)
2. **Stop services**: `docker-compose down`
3. **Update configuration** (already done)
4. **Restart services**: `docker-compose up -d`
5. **Verify new structure**: `./scripts/create_minio_buckets.sh`

## Monitoring and Verification

### MinIO Console

- URL: http://localhost:9001
- Credentials: `minioadmin` / `minioadmin`
- Should show single `data-lake` bucket

### Expected Structure

```bash
# List bucket contents
mc ls myminio/data-lake/

# Should show folders as they're created:
# PRE warehouse/
# PRE checkpoints/
# (other folders appear as data is written)
```

## Next Steps

1. **Test streaming pipeline** with new bucket structure
2. **Update Dagster assets** to use new paths (if needed)
3. **Create monitoring dashboards** for unified bucket metrics
4. **Implement data retention policies** at folder level
5. **Add data catalog integration** for improved discoverability
